{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-01-27T19:25:18.781392Z",
     "iopub.status.busy": "2022-01-27T19:25:18.780765Z",
     "iopub.status.idle": "2022-01-27T19:25:21.214753Z",
     "shell.execute_reply": "2022-01-27T19:25:21.213929Z",
     "shell.execute_reply.started": "2022-01-27T19:25:18.781263Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E:/pasca/Documents/Test/building_metadata.csv\n",
      "E:/pasca/Documents/Test/sample_submission.csv\n",
      "E:/pasca/Documents/Test/test.csv\n",
      "E:/pasca/Documents/Test/train.csv\n",
      "E:/pasca/Documents/Test/weather_test.csv\n",
      "E:/pasca/Documents/Test/weather_train.csv\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('E:/pasca/Documents/Test/'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "import os, gc, warnings\n",
    "import random\n",
    "import datetime\n",
    "import warnings\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "register_matplotlib_converters()\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import sklearn\n",
    "from sklearn import metrics\n",
    "import category_encoders\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import pickle\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:25:21.219093Z",
     "iopub.status.busy": "2022-01-27T19:25:21.218814Z",
     "iopub.status.idle": "2022-01-27T19:25:36.903564Z",
     "shell.execute_reply": "2022-01-27T19:25:36.902738Z",
     "shell.execute_reply.started": "2022-01-27T19:25:21.219057Z"
    }
   },
   "outputs": [],
   "source": [
    "#data import\n",
    "path = \"E:/pasca/Documents/Test/\"\n",
    "path_lap = \"C:/Users/Pascal Appelbaum/Documents/Test/\"\n",
    "train_df = pd.read_csv(path + 'train.csv')\n",
    "building_df = pd.read_csv(path + 'building_metadata.csv')\n",
    "weather_df = pd.read_csv(path + 'weather_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:25:36.905025Z",
     "iopub.status.busy": "2022-01-27T19:25:36.904802Z",
     "iopub.status.idle": "2022-01-27T19:25:37.066127Z",
     "shell.execute_reply": "2022-01-27T19:25:37.065267Z",
     "shell.execute_reply.started": "2022-01-27T19:25:36.905000Z"
    }
   },
   "outputs": [],
   "source": [
    "# Original code from https://www.kaggle.com/aitude/ashrae-missing-weather-data-handling by @aitude\n",
    "\n",
    "def fill_weather_dataset(weather_df):\n",
    "    \n",
    "    # Find Missing Dates\n",
    "    time_format = \"%Y-%m-%d %H:%M:%S\"\n",
    "    start_date = datetime.datetime.strptime(weather_df['timestamp'].min(),time_format)\n",
    "    end_date = datetime.datetime.strptime(weather_df['timestamp'].max(),time_format)\n",
    "    total_hours = int(((end_date - start_date).total_seconds() + 3600) / 3600)\n",
    "    hours_list = [(end_date - datetime.timedelta(hours=x)).strftime(time_format) for x in range(total_hours)]\n",
    "\n",
    "    missing_hours = []\n",
    "    for site_id in range(16):\n",
    "        site_hours = np.array(weather_df[weather_df['site_id'] == site_id]['timestamp'])\n",
    "        new_rows = pd.DataFrame(np.setdiff1d(hours_list,site_hours),columns=['timestamp'])\n",
    "        new_rows['site_id'] = site_id\n",
    "        weather_df = pd.concat([weather_df,new_rows])\n",
    "\n",
    "        weather_df = weather_df.reset_index(drop=True)           \n",
    "\n",
    "    # Add new Features\n",
    "    weather_df[\"datetime\"] = pd.to_datetime(weather_df[\"timestamp\"])\n",
    "    weather_df[\"day\"] = weather_df[\"datetime\"].dt.day\n",
    "    weather_df[\"week\"] = weather_df[\"datetime\"].dt.week\n",
    "    weather_df[\"month\"] = weather_df[\"datetime\"].dt.month\n",
    "    \n",
    "    # Reset Index for Fast Update\n",
    "    weather_df = weather_df.set_index(['site_id','day','month'])\n",
    "\n",
    "    air_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\n",
    "    weather_df.update(air_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    cloud_coverage_filler = weather_df.groupby(['site_id','day','month'])['cloud_coverage'].mean()\n",
    "    # Step 2\n",
    "    cloud_coverage_filler = pd.DataFrame(cloud_coverage_filler.fillna(method='ffill'),columns=[\"cloud_coverage\"])\n",
    "\n",
    "    weather_df.update(cloud_coverage_filler,overwrite=False)\n",
    "\n",
    "    due_temperature_filler = pd.DataFrame(weather_df.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\n",
    "    weather_df.update(due_temperature_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    sea_level_filler = weather_df.groupby(['site_id','day','month'])['sea_level_pressure'].mean()\n",
    "    # Step 2\n",
    "    sea_level_filler = pd.DataFrame(sea_level_filler.fillna(method='ffill'),columns=['sea_level_pressure'])\n",
    "\n",
    "    weather_df.update(sea_level_filler,overwrite=False)\n",
    "\n",
    "    wind_direction_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_direction'].mean(),columns=['wind_direction'])\n",
    "    weather_df.update(wind_direction_filler,overwrite=False)\n",
    "\n",
    "    wind_speed_filler =  pd.DataFrame(weather_df.groupby(['site_id','day','month'])['wind_speed'].mean(),columns=['wind_speed'])\n",
    "    weather_df.update(wind_speed_filler,overwrite=False)\n",
    "\n",
    "    # Step 1\n",
    "    precip_depth_filler = weather_df.groupby(['site_id','day','month'])['precip_depth_1_hr'].mean()\n",
    "    # Step 2\n",
    "    precip_depth_filler = pd.DataFrame(precip_depth_filler.fillna(method='ffill'),columns=['precip_depth_1_hr'])\n",
    "\n",
    "    weather_df.update(precip_depth_filler,overwrite=False)\n",
    "\n",
    "    weather_df = weather_df.reset_index()\n",
    "    weather_df = weather_df.drop(['datetime','day','week'],axis=1)\n",
    "        \n",
    "    return weather_df\n",
    "\n",
    "# Original code from https://www.kaggle.com/gemartin/load-data-reduce-memory-usage by @gemartin\n",
    "\n",
    "from pandas.api.types import is_datetime64_any_dtype as is_datetime\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "def reduce_mem_usage(df, use_float16=False):\n",
    "    \"\"\"\n",
    "    Iterate through all the columns of a dataframe and modify the data type to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    \n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage of dataframe is {:.2f} MB\".format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n",
    "            continue\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == \"int\":\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype(\"category\")\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print(\"Memory usage after optimization is: {:.2f} MB\".format(end_mem))\n",
    "    print(\"Decreased by {:.1f}%\".format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def features_engineering(df):\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df.sort_values(\"timestamp\")\n",
    "    df.reset_index(drop=True)\n",
    "    \n",
    "    # Add more features\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"],format=\"%Y-%m-%d %H:%M:%S\")\n",
    "    df[\"hour\"] = df[\"timestamp\"].dt.hour\n",
    "    df[\"weekend\"] = df[\"timestamp\"].dt.weekday\n",
    "    holidays = [\"2016-01-01\", \"2016-01-18\", \"2016-02-15\", \"2016-05-30\", \"2016-07-04\",\n",
    "                    \"2016-09-05\", \"2016-10-10\", \"2016-11-11\", \"2016-11-24\", \"2016-12-26\",\n",
    "                    \"2017-01-02\", \"2017-01-16\", \"2017-02-20\", \"2017-05-29\", \"2017-07-04\",\n",
    "                    \"2017-09-04\", \"2017-10-09\", \"2017-11-10\", \"2017-11-23\", \"2017-12-25\",\n",
    "                    \"2018-01-01\", \"2018-01-15\", \"2018-02-19\", \"2018-05-28\", \"2018-07-04\",\n",
    "                    \"2018-09-03\", \"2018-10-08\", \"2018-11-12\", \"2018-11-22\", \"2018-12-25\",\n",
    "                    \"2019-01-01\"]\n",
    "    df[\"is_holiday\"] = (df.timestamp.isin(holidays)).astype(int)\n",
    "    df['square_feet'] =  np.log1p(df['square_feet']**0.5)\n",
    "    \n",
    "    # Remove Unused Columns\n",
    "    #drop = [\"timestamp\",\"sea_level_pressure\", \"wind_direction\", \"wind_speed\",\"year_built\",\"floor_count\"]\n",
    "    #df = df.drop(drop, axis=1)\n",
    "    #gc.collect()\n",
    "    \n",
    "    # Encode Categorical Data\n",
    "    #le = LabelEncoder()\n",
    "    #df[\"primary_use\"] = le.fit_transform(df[\"primary_use\"])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:25:37.067476Z",
     "iopub.status.busy": "2022-01-27T19:25:37.067238Z",
     "iopub.status.idle": "2022-01-27T19:26:00.174519Z",
     "shell.execute_reply": "2022-01-27T19:26:00.173920Z",
     "shell.execute_reply.started": "2022-01-27T19:25:37.067447Z"
    }
   },
   "outputs": [],
   "source": [
    "#Datafiller\n",
    "weather_df = fill_weather_dataset(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:00.176607Z",
     "iopub.status.busy": "2022-01-27T19:26:00.176236Z",
     "iopub.status.idle": "2022-01-27T19:26:02.750558Z",
     "shell.execute_reply": "2022-01-27T19:26:02.749693Z",
     "shell.execute_reply.started": "2022-01-27T19:26:00.176570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 616.95 MB\n",
      "Memory usage after optimization is: 173.84 MB\n",
      "Decreased by 71.8%\n",
      "Memory usage of dataframe is 0.07 MB\n",
      "Memory usage after optimization is: 0.02 MB\n",
      "Decreased by 73.9%\n",
      "Memory usage of dataframe is 10.72 MB\n",
      "Memory usage after optimization is: 2.73 MB\n",
      "Decreased by 74.5%\n"
     ]
    }
   ],
   "source": [
    "#Mem reduce\n",
    "train_df = reduce_mem_usage(train_df,use_float16=True)\n",
    "building_df = reduce_mem_usage(building_df,use_float16=True)\n",
    "weather_df = reduce_mem_usage(weather_df,use_float16=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:02.751888Z",
     "iopub.status.busy": "2022-01-27T19:26:02.751668Z",
     "iopub.status.idle": "2022-01-27T19:26:10.461185Z",
     "shell.execute_reply": "2022-01-27T19:26:10.460279Z",
     "shell.execute_reply.started": "2022-01-27T19:26:02.751860Z"
    }
   },
   "outputs": [],
   "source": [
    "#Merge Data\n",
    "train_df = train_df.merge(building_df, left_on='building_id',right_on='building_id',how='left')\n",
    "train_df = train_df.merge(weather_df,how='left',left_on=['site_id','timestamp'],right_on=['site_id','timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:25.614344Z",
     "iopub.status.busy": "2022-01-27T19:26:25.614036Z",
     "iopub.status.idle": "2022-01-27T19:26:36.838627Z",
     "shell.execute_reply": "2022-01-27T19:26:36.837793Z",
     "shell.execute_reply.started": "2022-01-27T19:26:25.614298Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove Buildings\n",
    "train_df.drop(train_df.loc[train_df['building_id']== 1099].index, inplace=True)\n",
    "train_df.drop(train_df.loc[train_df['building_id']== 778].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:36.840236Z",
     "iopub.status.busy": "2022-01-27T19:26:36.839846Z",
     "iopub.status.idle": "2022-01-27T19:26:36.845825Z",
     "shell.execute_reply": "2022-01-27T19:26:36.845061Z",
     "shell.execute_reply.started": "2022-01-27T19:26:36.840202Z"
    }
   },
   "outputs": [],
   "source": [
    "#remove columns\n",
    "del train_df[\"year_built\"]\n",
    "del train_df[\"floor_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"timestamp\"] = train_df[\"timestamp\"].astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove zero Values\n",
    "train_df = train_df.query('not (building_id <= 104 & meter == 0 & timestamp <= \"2016-05-20\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:10.463034Z",
     "iopub.status.busy": "2022-01-27T19:26:10.462493Z",
     "iopub.status.idle": "2022-01-27T19:26:25.612213Z",
     "shell.execute_reply": "2022-01-27T19:26:25.611252Z",
     "shell.execute_reply.started": "2022-01-27T19:26:10.462973Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df = features_engineering(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:39.927846Z",
     "iopub.status.busy": "2022-01-27T19:26:39.927454Z",
     "iopub.status.idle": "2022-01-27T19:26:40.587497Z",
     "shell.execute_reply": "2022-01-27T19:26:40.586676Z",
     "shell.execute_reply.started": "2022-01-27T19:26:39.927799Z"
    }
   },
   "outputs": [],
   "source": [
    "target = np.log1p(train_df[\"meter_reading\"])\n",
    "train = train_df.drop([\"building_id\", 'meter_reading', 'timestamp','hour', \"weekend\", \"is_holiday\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:40.588866Z",
     "iopub.status.busy": "2022-01-27T19:26:40.588640Z",
     "iopub.status.idle": "2022-01-27T19:26:45.022858Z",
     "shell.execute_reply": "2022-01-27T19:26:45.022121Z",
     "shell.execute_reply.started": "2022-01-27T19:26:40.588838Z"
    }
   },
   "outputs": [],
   "source": [
    "train_1 = pd.get_dummies(train, columns = ['meter', \"site_id\", \"primary_use\",'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_train, target_test, data_train, data_test = train_test_split(target,train_1, test_size = .20, random_state= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-01-27T19:26:45.024717Z",
     "iopub.status.busy": "2022-01-27T19:26:45.024166Z",
     "iopub.status.idle": "2022-01-27T19:27:19.485298Z",
     "shell.execute_reply": "2022-01-27T19:27:19.484356Z",
     "shell.execute_reply.started": "2022-01-27T19:26:45.024679Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LinearRegression().fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy %d', model.score(data_train, target_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pred = np.expm1(model.predict(data_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy %d', model.score(data_test, target_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_absolute_error(target_test,target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.mean_squared_error(target_test,target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sqrt(metrics.mean_squared_error(target_test,target_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('R^2 =',metrics.explained_variance_score(target_test,target_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.linalg.norm(np.log1p(target_pred) - np.log1p(target_test)) / np.sqrt(len(target_test))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cdf = pd.DataFrame(data = model.coef_, index = train_1.columns, columns = ['Coefficients'])\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,15))\n",
    "cdf.Coefficients.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = np.linalg.norm(target_pred - target_test) / np.sqrt(len(target_test))\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,v in enumerate(importance):\n",
    "\tprint('Feature: %0d, Score: %.5f' % (i,v))\n",
    "# plot feature importance\n",
    "plt.bar([x for x in range(len(importance))], importance)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
